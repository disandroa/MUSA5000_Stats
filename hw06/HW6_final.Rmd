---
title: "Text Analysis of IMDb Movie Reviews"
author: "Akira Di Sandro, Sofia Fasullo, Amy Solano"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
  pdf_document:
    toc: true
---

The aim of this Markdown is to demonstrate the use of various text analysis tools in R, including text clustering, word clouds and sentiment analysis.

## Data Description

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#setwd("~/Documents/GitHub/MUSA5000_Stats/hw06")

library(wordcloud)
#library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(syuzhet)

knitr::opts_chunk$set(echo = T, eval = T, warning = F, message = F)

# get rid of scientific notation
options(scipen = 999)
```


```{r load-data}
imdb_df <- read.csv("Data/IMDB_Dataset.csv")
```

```{r create_string}
imdb_review_text <- imdb_df$review
```

## Introduction

## Methods

### Data Preprocessing

#### Corpus

First, we want to convert our dataframe of movie reviews into a corpus. A text corpus (plural: _corpora_) "is a large and unstructured set of texts (nowadays usually electronically stored and processed) used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.” (https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/language-corpora) In this assignment, we utilize the r package..t…m..[fix later] for converting our data to a corpus and the subsequent preprocessing and analyses. Once we have created the corpus, we also make sure to convert everything to lowercase using `content_transformer`  so that our analysis is not case-sensitive.

#### Punctuation and Special Characters

Since our corpus entries all consist of a single line, concatenation — the process of joining separate strings into a single line — is not necessary and thus we do not perform it, though this step may be important for preprocessing corpora that include line breaks. The next necessary step once we have created our corpus and converted all words to lowercase is to remove special characters, numbers, and punctuation marks. 

To do so, we create functions `toSpace` and `remApostrophe` that take special characters (e.g., **@**, **/**, **]**, **$**) as arguments, identifying instances of these characters in our corpus entries and replacing them with spaces. Other characters, like numbers and punctuation marks, can be removed using the `removeNumbers` and `removePunctuation` arguments within the `tm_map` function.

#### Stop Words and Word Grouping

After removing punctuation and special characters, we also make sure to get rid of **stop words**. In the English language, there are many stop words such as, “as”, “and”, “in”, and “for”, which are commonly used words that don’t add useful information to the text overall. For this step, we used a predefined set of stop words from the `Snowball` package in R. We also removed several other neutral and common words in this corpus including words like, “movie”, “feature”, “screen”, “imdb”, and “film”.

**Stemming** is a technique that helps us group words that have the same stem into the same word. For example, the process of stemming allows us to transform words like “singing”, “singer”, and “singers” to the stem word, “sing”. In our analyses, we use the `stemDocument()` function from the `tm` package, which uses Porter’s stemming algorithm, removing all suffixes to obtain the stem.

A shortcoming of stemming is the possibility that the ends of the words will be incorrectly or incompletely removed. In spite of this, this assignment favors stemming over other word-grouping techniques due to the simplicity of the operation for the purpose of analyzing movie reviews. Abnormal results of stemming are noted in the “Results” section. 

Similar to stemming, **lemmatization** also groups similar words together in a corpus. Unlike stemming, which groups multiple forms of a word together by removing the suffix, lemmatization searches for a word’s _lemma_, or base or dictionary form, by conducting a more complete analysis of the words in the corpus (https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html). This makes lemmatization more computationally demanding than stemming, but depending on the analysis, the performance differences are modeset. For this assignment, stemming is the better choice for grouping like words.


## Results
```{r warning=FALSE, message=FALSE, cache=FALSE}
# Load and preprocess all text documents
myCorpus <- tm::VCorpus(VectorSource(imdb_review_text))

# Convert everything to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
```

```{r check_cat, warning=FALSE, message=FALSE, cache=FALSE}

line_2 <- numeric(50000)
for (i in 1:50000) {
  # Check if the second element in myCorpus[[i]] is NA - therefore review is one line
  obj <- ifelse(is.na(content(myCorpus[[i]])[2]), 0, 1)
  line_2[i] <- obj
}
```
### Data cleaning
```{r warning=FALSE, message=FALSE, cache=FALSE}
#     Defining the toSpace function
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#     Defining the remApostrophe function
remApostrophe <- content_transformer(function(x,pattern) gsub(pattern, "", x))
#     Removing special characters
myCorpus <- tm_map(myCorpus, toSpace, "@")
myCorpus <- tm_map(myCorpus, toSpace, "</>")
myCorpus <- tm_map(myCorpus, toSpace, "/")
myCorpus <- tm_map(myCorpus, toSpace, "]")
myCorpus <- tm_map(myCorpus, toSpace, "$")
myCorpus <- tm_map(myCorpus, toSpace, "—")
myCorpus <- tm_map(myCorpus, toSpace, "‐")
myCorpus <- tm_map(myCorpus, toSpace, "”")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, toSpace, "“")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, remApostrophe, "’")

myCorpus <- tm::tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removePunctuation)

# remove stop words
stopwords("english")
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
```
#### Document lengths, stemming, etc
```{r warning=FALSE, message=FALSE, cache=FALSE}
# removing self-defined stop words
myCorpus <- tm_map(myCorpus, removeWords,c("movie", "film", "imdb", 
                                           "review", "cinema", "theater", "feature", "screen"))
```

```{r warning=FALSE, message=FALSE, cache=FALSE, eval=FALSE}
myCorpus <- tm_map(myCorpus, stemDocument)
```

```{r warning=FALSE, message=FALSE, cache=FALSE}

# Calculate word counts for all documents
word_counts <- sapply(myCorpus, function(doc) {
  length(unlist(strsplit(as.character(doc), "\\s+")))
})

word_count_df <- data.frame(word_counts = word_counts)

ggplot(word_count_df, aes(x = word_counts)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Word Counts per Document", 
       x = "Word Count", 
       y = "Number of Documents") +
  theme_minimal()
```

```{r}
max_words =  max(word_count_df$word_counts)
min_words = min(word_count_df$word_counts)
sentence = paste("The shortest reviews had", min_words, "words and the longest reviews had", max_words, "words.")
print(sentence)
```

```{r warning=FALSE, message=FALSE, cache=FALSE}
dtm <- DocumentTermMatrix(myCorpus)

dtm_reduced <- removeSparseTerms(dtm, sparse = 0.995)  # Retain terms present in at least 0.5% of documents, so at least 250 reviews

m <- as.matrix(dtm_reduced)

```

```{r warning=FALSE, message=FALSE, cache=FALSE}

cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
rownames(cs) <- dtm_reduced$dimnames$Terms

hist(cs, breaks=100,main = "Histogram of word frequencies",xlab="Repition of word across all reviews",ylab="Number of words")                    #Let's look at some histograms/tabulations/word cloud of total term appearance. 

```
### Word Cloud
Some words occur in the 10s of thousands - but not all look like fillers.

```{r}

#AI says these are fillers: 
#one, like, just, even, really, well, also, much, still, say, now, though, actually, pretty, quite, however, may
#but I think they may be important to show a reviewers feelings or hesitation

myCorpus <- tm_map(myCorpus, removeWords,c("one", "like", "just", "even", "time", "really", " see", "can", "much", "well", "get", "will", "also", "first", "dont", "movies", "made", "make", "films", "way", "watch", "many", "seen", "two", "character", "never", "know", "ever", "still", "say", "end", "something", "back", "watching", "thing", "doesnt", "now", "didnt", "years", "another", "though", "actually", "makes", "nothing", "find", "look", "going", "work", "lot", "every", "part", "cant", "want", "quite", "things", "seems", "around", "got", "take", "however", "fact", "give", "thought", "ive", "may", "without", "saw"))

```
So the threshold will be set at 5000, a nice even number to limit the wordcloud to only the most used words

```{r wordcloud}

wordcloud(myCorpus, min.freq=5000)
```

### Sentiment Analysis

```{r warning=FALSE, message=FALSE, cache=FALSE}
nrc <- syuzhet::get_sentiment_dictionary(dictionary="nrc")
afinn <- syuzhet::get_sentiment_dictionary(dictionary="afinn")
bing <- syuzhet::get_sentiment_dictionary(dictionary="bing")
syuzhet <- syuzhet::get_sentiment_dictionary(dictionary="syuzhet")
```

```{r}
# shortest reviews
word_count_asc <- word_count_df %>% 
  arrange(word_counts)

short30_index <- as.numeric(rownames(word_count_asc)[1:30])

short30 <- m[short30_index,]

# longest reviews
word_count_desc <- word_count_df %>% 
  arrange(desc(word_counts))


long30_index <- as.numeric(rownames(word_count_desc)[1:30])
long30 <- m[long30_index,]
```


```{r warning=FALSE, message=FALSE, cache=FALSE}

longdf30 <- data.frame(col = rownames(long30)[row(long30)],
           Term = colnames(long30)[col(long30)], value = c(long30))
longdf30$afinn <- as.matrix(get_sentiment(longdf30$Term, method="afinn"))
longdf30.scores <- longdf30 %>% 
  group_by(col) %>% 
  summarize(scorenet=sum(value*afinn),len=sum(value), score=scorenet/len)

ggplot(longdf30.scores) +
  geom_col(aes(x=col, y=scorenet), fill = "skyblue", color = "black")+
  labs(title="Net Sentiment Scores for 30 longest reviews", 
  subtitle="Scores from AFINN lexicon",
  x=NULL, y="Sentiment Score")+
  theme_minimal()+
  theme(axis.text.x = element_blank())

```

``` {r warning=FALSE, message=FALSE, cache=FALSE}
shortdf30 <- data.frame(col = rownames(short30)[row(short30)],
           Term = colnames(short30)[col(short30)], value = c(short30))
shortdf30$afinn <- as.matrix(get_sentiment(shortdf30$Term, method="afinn"))
shortdf30.scores <- shortdf30 %>% 
  group_by(col) %>% 
  summarize(scorenet=sum(value*afinn),len=sum(value), score=scorenet/len)

ggplot(shortdf30.scores) +
  geom_col(aes(x=col, y=scorenet), fill = "skyblue", color = "black")+
  labs(title="Net Sentiment Scores for 30 shortest reviews", 
  subtitle="Scores from AFINN lexicon",
  x=NULL, y="Sentiment Score")+
  theme_minimal()+
  theme(axis.text.x = element_blank())
```

## Discussion

## References

1. Hill, Chelsey. 2023. "Sentiment Analysis (Lexicons)". Rstudio-Pubs-Static.S3.Amazonaws.Com. https://rstudio-pubs-static.s3.amazonaws.com/676279_2fa8c2a7a3da4e7089e24442758e9d1b.html.

2. "Sentiment Analysis In R | R-Bloggers". 2021. R-Bloggers. https://www.r-bloggers.com/2021/05/sentiment-analysis-in-r-3/.

3. Robinson, Julia. 2023. "2 Sentiment Analysis With Tidy Data | Text Mining With R". Tidytextmining.Com. https://www.tidytextmining.com/sentiment.html.

4. "Text Mining: Sentiment Analysis · AFIT Data Science Lab R Programming Guide ". 2023. Afit-R.Github.Io. https://afit-r.github.io/sentiment_analysis.

5. "TDM (Term Document Matrix) And DTM (Document Term Matrix)". 2023. Medium. https://medium.com/analytics-vidhya/tdm-term-document-matrix-and-dtm-document-term-matrix-8b07c58957e2.

6. "Text Clustering With R: An Introduction For Data Scientists". 2018. Medium. https://medium.com/@SAPCAI/text-clustering-with-r-an-introduction-for-data-scientists-c406e7454e76.

7. "Introductory Tutorial To Text Clustering With R". 2023. Rstudio-Pubs-Static.S3.Amazonaws.Com. https://rstudio-pubs-static.s3.amazonaws.com/445820_c6663e5a79874afdae826669a9499413.html.

8. "Library Guides: Text Mining & Text Analysis: Language Corpora". 2023. Guides.Library.Uq.Edu.Au. https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/language-corpora.

9. Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, "Introduction to Information Retrieval", Cambridge University Press. 2008. https://www-nlp.stanford.edu/IR-book/




